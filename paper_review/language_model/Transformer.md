> ë…¼ë¬¸ì„ ì½ê³  ë°°ìš´ ë‚´ìš©ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì˜ëª»ëœ ë‚´ìš©ì´ë‚˜ ì„¤ëª…ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
í•‘í ë¸”ë¡œê·¸ì˜ [ê¸€](https://blog.pingpong.us/transformer-review/)ë„ ì°¸ê³ í•˜ì—¬ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.


## Title  
Attention Is All You Need
 [Link](https://arxiv.org/abs/1706.03762)  

## Year
2017

## Authors  
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin

## Abstract
ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•œ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ê°€ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
The best performing models also connect the encoder and decoder through an attention mechanism.

ìœ„ ëª¨ë¸ì— ë¹„í•´ ë³¸ ì—°êµ¬ì—ì„œ ê°œë°œí•œ ëª¨ë¸ì€ ì˜ì–´-ë…ì¼ì–´ ë²ˆì—­ ì‘ì—…ì—ì„œ 2BLEU ì´ìƒì˜ ì„±ëŠ¥ í–¥ìƒì´ ìˆì—ˆë‹¤. 

## Introduction  
Encoder-Decoder êµ¬ì¡°ì˜ NMTê°€ ìµœê·¼ ì£¼ëª©ë°›ê³  ìˆë‹¤. í•˜ì§€ë§Œ fixed-length-vectorì— ì†ŒìŠ¤ ë¬¸ì¥ì— ëª¨ë“  ì •ë³´ë¥¼ ì••ì¶•(compress)í•˜ë‹¤ ë³´ë‹ˆ ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ì— ì·¨ì•½í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. 

ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ align and translate jointlyí•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ì˜ í™•ì¥(extension)ì„ ì œì•ˆí•œë‹¤.

In order to address this issue, we introduce an extension to the encoderâ€“decoder model which learns to align and translate jointly.  

ë²ˆì—­ëœ ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œë§ˆë‹¤ ëª¨ë¸ì€ **ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ê°€ ì§‘ì¤‘ëœ ì†ŒìŠ¤ ë¬¸ì¥ì˜ ìœ„ì¹˜ë¥¼ ê²€ìƒ‰**í•œë‹¤.  ê·¸ë¦¬ê³  ëª¨ë¸ì€ ì†ŒìŠ¤ ë¬¸ì¥ì˜ ìœ„ì¹˜ì™€ context vector ê¸°ë°˜ìœ¼ë¡œ target word(ë²ˆì—­ëœ ë‹¨ì–´)ë¥¼ ì˜ˆì¸¡í•œë‹¤. 



Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.  

ğŸ‘‰ ê¸°ì¡´ì˜ Fixed-length vector ì •ë³´ë§Œì„ í™œìš©í•˜ëŠ” ë°©ì‹ë³´ë‹¤ ë°œì „.  
ğŸ‘‰ ê¸´ ë¬¸ì¥ì— ë”ìš± ì˜ ëŒ€ì²˜í•  ìˆ˜ ìˆë‹¤.  


The most important distinguishing feature of this approach from the basic encoderâ€“decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. We show this allows a model to cope better with long sentences.