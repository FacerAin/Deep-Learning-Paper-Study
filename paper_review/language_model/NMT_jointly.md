> ë…¼ë¬¸ì„ ì½ê³  ë°°ìš´ ë‚´ìš©ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì˜ëª»ëœ ë‚´ìš©ì´ë‚˜ ì„¤ëª…ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Title  
Neural Machine Translation by Jointly Learning to Align and Translate [Link](https://arxiv.org/abs/1409.0473)  

## Year
2014 - 2016

## Authors  
Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio  

## Abstract

ìµœê·¼ ì‹ ê²½ë§ ê¸°ê³„ ë²ˆì—­(Neural Machine Translation)ì€ ê¸°ê³„ ë²ˆì—­ì—ì„œ íš¨ê³¼ì ì¸ ë°©ë²•ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆë‹¤.  
ğŸ‘‰ Seq2Seq ëª¨ë¸ë¡œ ëŒ€í‘œë˜ëŠ” ì‹ ê²½ë§ ê¸°ê³„ë²ˆì—­ì´ ëŒ€ì„¸. í•˜ì§€ë§Œ Long Term Dependency ë“±ì˜ ë¬¸ì œë„ ì¡´ì¬.

Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance.  

ë³¸ ì—°êµ¬ì—ì„œëŠ” ê³ ì • ê¸¸ì´ ë²¡í„°(Fixed-length Vector)ê°€ ì•„í‚¤í…ì²˜ ì„±ëŠ¥ í–¥ìƒì— ë³‘ëª© í˜„ìƒ(Bottleneck)ì´ë¼ ì¶”ì¸¡í•˜ê³ , ëª¨ë¸ì´ ëŒ€ìƒ ë‹¨ì–´ ì˜ˆì¸¡ê³¼ ê´€ë ¨ëœ ì†ŒìŠ¤ ë¬¸ì¥ì˜ ë¶€ë¶„ì„ ìë™ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ í™•ì¥í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.  
ğŸ‘‰ Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ê¸°ë³¸ ê°œë…, ëŒ€ìƒ ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ì°¸ê³ í•˜ì! 


In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoderâ€“decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search
for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.  

ìƒˆë¡œìš´ ì ‘ê·¼ì„ í†µí•´ ì˜ì–´-í”„ë‘ìŠ¤ì–´ ë²ˆì—­ ì‘ì—…ì„ state-of-art ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤. ë˜í•œ qualitative analysisì— ì˜í•´ ëª¨ë¸ì˜ ì¡°ì •ì€ ì¸ê°„ì˜ ì§ê´€ê³¼ ìœ ì‚¬í•¨ì„ ë°œê²¬í•˜ì˜€ë‹¤.  

With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.

## Introduction  
Encoder-Decoder êµ¬ì¡°ì˜ NMTê°€ ìµœê·¼ ì£¼ëª©ë°›ê³  ìˆë‹¤. í•˜ì§€ë§Œ fixed-length-vectorì— ì†ŒìŠ¤ ë¬¸ì¥ì— ëª¨ë“  ì •ë³´ë¥¼ ì••ì¶•(compress)í•˜ë‹¤ ë³´ë‹ˆ ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ì— ì·¨ì•½í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. 

ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ align and translate jointlyí•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ì˜ í™•ì¥(extension)ì„ ì œì•ˆí•œë‹¤.

In order to address this issue, we introduce an extension to the encoderâ€“decoder model which learns to align and translate jointly.  

ë²ˆì—­ëœ ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œë§ˆë‹¤ ëª¨ë¸ì€ **ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ê°€ ì§‘ì¤‘ëœ ì†ŒìŠ¤ ë¬¸ì¥ì˜ ìœ„ì¹˜ë¥¼ ê²€ìƒ‰**í•œë‹¤.  ê·¸ë¦¬ê³  ëª¨ë¸ì€ ì†ŒìŠ¤ ë¬¸ì¥ì˜ ìœ„ì¹˜ì™€ context vector ê¸°ë°˜ìœ¼ë¡œ target word(ë²ˆì—­ëœ ë‹¨ì–´)ë¥¼ ì˜ˆì¸¡í•œë‹¤. 



Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.  

ğŸ‘‰ ê¸°ì¡´ì˜ Fixed-length vector ì •ë³´ë§Œì„ í™œìš©í•˜ëŠ” ë°©ì‹ë³´ë‹¤ ë°œì „.  
ğŸ‘‰ ê¸´ ë¬¸ì¥ì— ë”ìš± ì˜ ëŒ€ì²˜í•  ìˆ˜ ìˆë‹¤.  


The most important distinguishing feature of this approach from the basic encoderâ€“decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. We show this allows a model to cope better with long sentences.