> 논문을 읽고 배운 내용을 정리한 글입니다. 잘못된 내용이나 설명이 있을 수 있습니다.

## Title

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [Link](https://arxiv.org/abs/1810.04805)

## Year

2018

## Authors

Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova

## Abstract

새로운 Language Representation Model인 **BERT**(**B**idirectional **E**ncoder **R**epresentations from **T**ransformers)를 제안한다.

BERT는 모든 계층에서 왼쪽과 오른쪽 context를 jointly conditioning(공동으로 조절) 함으로써 라벨이 없는 텍스트에서 깊은 양방향 representation을 Pre-Train하도록 설계되었다.
