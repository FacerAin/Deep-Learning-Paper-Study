> 논문을 읽다 궁금한 Question Keyword를 정리한 글입니다. 잘못된 내용이나 설명이 있을 수 있습니다.

## Question Keyword

1. Attention
2. Transformer
3. BERT

> 김기현의 자연어 처리 딥러닝 캠프(김기현 지음) 서적과 [딥러닝 자연어 처리 입문](https://wikidocs.net/22893) 포스트를 바탕으로 작성했습니다.

## Attention

> [Ratsgo's blog](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/10/06/attention/) 님의 글을 참고하였습니다.

Seq2Seq 모델은 오토인코더(Auto Encoder)의 일종으로 시계열 또는 시퀀스 데이터에 강점이 있다.
하지만 기존 Seq2Seq 모델은 정보 손실의 한계를 가지고 있다.

- 하나의 고정된 크기의 벡터에 정보를 압축하려다보니 **정보의 손실이 발생**한다. 이 때문에 긴 문장에 대해서는 번역의 품질이 떨어진다.

따라서 디코더에서 단어를 출력하는 매 시점(Time Step)마다 인코더의 전체 입력 문장을 다시 한번 참고하여 성능을 개선한다.

> "예컨대 독일어 “Ich mochte ein bier”를 영어 “I’d like a beer”로 번역하는 S2S 모델을 만든다고 칩시다. 모델이 네번째 단어인 ‘beer’를 예측할 때 ‘bier’에 주목하게 만들고자 합니다. **어텐션 매커니즘의 가정은 인코더가 ‘bier’를 받아서 벡터로 만든 결과(인코더 출력)는 디코더가 ‘beer’를 예측할 때 쓰는 벡터(디코더 입력)와 유사할 것이라는 점입니다.**" by Ratsgo's blog

![](https://wikidocs.net/images/page/22893/dotproductattention1_final.PNG)
[이미지 출처 https://wikidocs.net/22893](https://wikidocs.net/22893)

아래는 어텐션을 표현한 그림이다.

![](https://wikidocs.net/images/page/22893/%EC%BF%BC%EB%A6%AC.PNG)  
[이미지 출처 https://wikidocs.net/22893](https://wikidocs.net/22893)

어텐션을 함수로 표현하면 아래와 같다.
Attention(Query, Keys, Values) = Attention Value
주어진 Query에 대해서 모든 Key의 유사도를 각각 구한다. 그리고 구해진 유사도를 각각의 Value에 반영해주고, 반영된 Value를 모두 더해서 리턴한다.

이때 각각의 값들이 모델에서 의미하는 바는 아래와 같다.

- Query: **t 시점의 디코더 셀**에서의 은닉 상태들
- Keys: **모든 시점의 인코더 셀**의 은닉 상태들
- Values: **모든 시점의 인코더 셀**의 은닉 상태들

이후 동작 과정에 대해서는 다음 [링크](https://wikidocs.net/22893)로 대체한다.

아래 그림은 어텐션 동작 과정을 요약한 것이다.

![](https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg)
[이미지 출처 Tensorflow Reference](https://www.tensorflow.org/tutorials/text/nmt_with_attention)

## Transformer

![](https://wikidocs.net/images/page/31379/transformer2.PNG)
![](https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG)

## BERT

[참고 자료](http://docs.likejazz.com/bert/)
