> 논문을 읽다 궁금한 Question Keyword를 정리한 글입니다. 잘못된 내용이나 설명이 있을 수 있습니다.

## Question Keyword

1. 자기회귀(Autoregressive, AR)
2. Teacher Forcing
3. NMT 탐색(추론)
> 김기현의 자연어 처리 딥러닝 캠프(김기현 지음) 서적과 [딥러닝 자연어 처리 입문](https://wikidocs.net/22893) 포스트를 바탕으로 작성했습니다.  


### 자기 회귀

![](https://6chaoran.files.wordpress.com/2019/01/encoder-decoder-architecture.png?w=700)
[출처](https://6chaoran.wordpress.com/2019/01/15/build-a-machine-translator-using-keras-part-1-seq2seq-with-lstm/)
과거의 자신의 값을 참조하여 현재의 값을 추론하는 특징

문제점  

-  과거에 잘못된 예측을 했을 경우 시간이 지날수록 더 큰 잘못된 예측을 할 가능성을 야기
- 과거의 결괏값에 따라 문장의 구성이 바뀔 뿐만 아니라, 예측 문장의 길이마저 바뀌므로 정답 학습이 힘듬.

### Tearcher Forcing
![](https://cdn-images-1.medium.com/max/1000/1*PFPSLEjIe152uR9UR59LDA.png)
[출처](https://mc.ai/translation-or-answer-tool-seq2seq-with-teacher-forcing-and-attention-mechanism/)

디코더를 학습시킬 때 예측한 값이 아닌 정답 데이터를 사용하자.

### 탐욕 탐색 알고리즘


소프트맥스 계층의 확률 분포대로 샘플링을 수행하는 방법은?
👉 같은 입력에 대해 매번 다른 출력 결과물을 만들어낼 수 있다.   

Geedy Algoritm을 적용하자.  
👉 미래를 생각하지 않고 **현재 단계에서 최선**을 선택을 한다.   
👉 각 소프트맥스 계층의 확률의 최대값을 사용한다.  
![](https://opennmt.net/OpenNMT/img/beam_search.png)
[출처](https://opennmt.net/OpenNMT/translation/beam_search/)  

### 빔서치 Beam Search
탐욕법은 최선의 결과를 보장하지는 않는다.  
따라서 k개의 후보를 더 추적하는데, 이때 k를 Beam의 크기라고 한다.  
이 방법을 Beam Search라고 한다.  

![](https://atcold.github.io/pytorch-Deep-Learning/images/week12/12-2/Beam_Decoding.png)
[출처](https://atcold.github.io/pytorch-Deep-Learning/ko/week12/12-2/)

